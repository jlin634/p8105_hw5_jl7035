---
title: "p8105_hw5_jl7035"
author: "Jeffrey Lin"
date: "2024-11-11"
output: github_document
---

# Load Libraries
```{r}
library(tidyverse)

```


# Problem 1

## Functions for Randomly Drawing Birthdays and calculating prob of duplicates

```{r}
dup_bdays = function(samp_size) {
  bday_vec <- sample(c(1:365), size = samp_size, replace = TRUE)

  if (n_distinct(bday_vec) < samp_size) {
    return(TRUE)
  } else {
    return(FALSE)
  } 
}

```

## Run Simulations from sample size 2 to 50
```{r}

prob_by_size <- c()

for (sample_size in 2:50){
  num_dups = 0
  for (iteration in 1:10000) {
    if (dup_bdays(sample_size))
      num_dups = num_dups + 1
  }
  
  avg_prob = num_dups / 10000
  prob_by_size = append(prob_by_size, values = avg_prob)
}

dup_bday_tbl <-
  tibble(
    sample_size = 2:50,
    avg_prob = prob_by_size
  )

```


## Plot of probability of duplicate birthdays by sample size
```{r}
dup_bday_tbl %>% 
  ggplot(aes(x = sample_size, y = avg_prob)) +
  geom_point() +
  xlab("Sample Size") +
  ylab("Probability of Duplicate Birthdays") +
  ggtitle("Probability of Duplicate Birthdays by Sample Size")


```
As the sample size increases, the probability of duplicate birthdays increases.
The probability of having duplicate birthdays grows above 0.50 around a sample
size of 23. 

# Problem 2


## Generate 5000 datasets for mu = 0 and save p-values
```{r}
n <- 30
sigma <- 5
pop_param <- 0

x_bar_vec <- c()
p_val_vec <- c()

for (iter in 1:5000) {
  rand_samp <- rnorm(n, pop_param, sigma)
  x_bar <- mean(rand_samp)
  x_bar_vec = c(x_bar_vec, values = x_bar)
  
  p_val <- 
    t.test(rand_samp, mu = 0, conf.level = 0.95) %>%
    broom::tidy() %>% 
    pull(p.value)
    
  p_val_vec = c(p_val_vec, p_val)
  
}

t_test_results <- tibble(
  samp_avg = x_bar_vec,
  p_val = p_val_vec
)


```


# Write Function that handles generating datasets and finding p-values for mu
```{r}
gen_test_data <- function(pop_param_vec, n, sigma, results) {
  x_bar_vec <- numeric()
  p_val_vec <- numeric()
  for (val in pop_param_vec) {
    for (iter in 1:5000) {
      rand_samp <- rnorm(n, val, sigma)
      x_bar <- mean(rand_samp)
      x_bar_vec <- c(x_bar_vec, x_bar)
      
      p_val <- 
      t.test(rand_samp, mu = 0, conf.level = 0.95) %>%
      broom::tidy() %>% 
      pull(p.value)
      p_val_vec <- c(p_val_vec, p_val)
      
    }
  }
  results <- 
    bind_rows(results, tibble(samp_avg = x_bar_vec, p_val = p_val_vec))
  return(results)
}

```

## Generate datasets and perform t-tests for mu values 1 to 5
```{r}
mu_vec <- c(1, 2, 3, 4, 5, 6)
t_test_results <- gen_test_data(mu_vec, n, sigma, t_test_results)

t_test_results <- t_test_results %>% 
  mutate(
    mu = ((row_number() -1) %/% 5000)
  )


```

## Proportion of Times Null was rejected in relation to true mean
```{r}
t_test_results %>% 
  filter(p_val < 0.05) %>% 
  ggplot(aes(x = factor(mu), fill = mu)) +
  geom_bar() +
  xlab("Number of t-tests with Null Rejected") +
  ylab("Total t-tests conducted") +
  ggtitle("Proportion of Null HYpohtesis Rejections by Population Mean")


```
Since we are conducting a t-test, where the supposed population mean is 0, 
we can see that as the actual value of mu increases, the proportion of 
null hypotheses rejected also increases. This suggests that as effect size 
increases, power increases. 
